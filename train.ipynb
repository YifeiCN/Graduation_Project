{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from functools import partial\n",
    "import math\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#jupyter 中开启该选项，否则不执行\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 配置超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_PATCH_SHAPE = (16, 16, 3)             # shape of input image patch: (W, H, C)  \n",
    "INPUT_PATCH_SIZE = 16                       # equal to W or H of INPUT_PATCH_SHAPE\n",
    "BATCH_SIZE = 256                            # \n",
    "BUFFER_SIZE = 60000                         # related to shuffle\n",
    "EPOCHS = 30                                 # try different EPOCHS and supervise in TensorBoard\n",
    "VALIDITION_SPLIT = 0.15                     # float between 0 and 1 : ratio of validation set to data set\n",
    "WEIGHT_DECAY = 1e-4                         # \n",
    "\n",
    "OUTPUT_FOLDER = '.\\\\model_output'           # the path saved ours models and logs\n",
    "IMG_DIR = '.\\\\trains'                       # training datasets path\n",
    "\n",
    "if not os.path.exists(OUTPUT_FOLDER):\n",
    "    os.mkdir(OUTPUT_FOLDER)\n",
    "    \n",
    "LOG_DIR = os.path.join(OUTPUT_FOLDER, 'logs_{}'.format(datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")))\n",
    "if not os.path.exists(LOG_DIR):\n",
    "    os.mkdir(LOG_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFO == 0：    | output : INFO + WARNING + ERROR + FATAL | ignore : nothing  \n",
    "# WARNING == 1:  | output : WARNING + ERROR + FATAL        | ignore : INFO\n",
    "# ERROR == 2:    | output : ERROR + FATAL                  | ignore : INFO + WARNING\n",
    "# FATAL == 3:    | output : FATAL                          | ignore : INFO + WARNING + ERROR\n",
    "# 不同的值设置的是基础的log信息（base_loging），运行时会输出base等级及之上的信息。\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 选择指定显卡及自动调用显存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')# 列出所有可见显卡\n",
    "print(\"All the available GPUs:\\n\", physical_devices)\n",
    "if physical_devices:\n",
    "    gpu = physical_devices[0] # 显示第一块显卡\n",
    "    tf.config.experimental.set_memory_growth(gpu, True) # 根据需要自动增长显存\n",
    "    tf.config.experimental.set_visible_devices(gpu, 'GPU') # 只选择第一块"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(img_dir, num_t, patch_size = INPUT_PATCH_SIZE, validation_split= VALIDITION_SPLIT):\n",
    "    \"\"\"\n",
    "    input:\n",
    "    img_dir: dir of haze-free images\n",
    "    num_t: a haze_free image patch generate num_t hazy image patches\n",
    "    patch_size: side length of image patch\n",
    "    validation_split: ratio of validation set to data set\n",
    "    \n",
    "    output:\n",
    "    \"\"\"\n",
    "     \n",
    "   \n",
    "    img_path = os.listdir(img_dir)\n",
    "    \n",
    "    random.shuffle(img_path)\n",
    "    \n",
    "    x_train = []\n",
    "    \n",
    "    y_train = []\n",
    "    \n",
    "\n",
    "    for image_name in img_path:\n",
    "        \n",
    "        fullname = os.path.join(img_dir, image_name)\n",
    "        \n",
    "        img = cv2.imread(fullname)\n",
    "        \n",
    "        w,h,_ = img.shape\n",
    "        \n",
    "        num_w = int(w / patch_size)\n",
    "        \n",
    "        num_h = int(h / patch_size)\n",
    "        \n",
    "        for i in range(num_w):\n",
    "            \n",
    "            for j in range(num_h):\n",
    "                \n",
    "                free_patch = img[0+i*patch_size:patch_size+i*patch_size, 0+j*patch_size:patch_size+j*patch_size, :]\n",
    "                \n",
    "                for k in range(num_t): \n",
    "                   \n",
    "                    t = random.random()                   \n",
    "                    \n",
    "                    hazy_patch = t * free_patch / np.float(255)  + (1 - t)         # Note that the image pixel value has been normalized here\n",
    "                    \n",
    "                    x_train.append(hazy_patch)\n",
    "                    \n",
    "                    y_train.append(t)\n",
    "                    \n",
    "\n",
    "    x_train = np.array(x_train)\n",
    "    \n",
    "    y_train = np.array(y_train)\n",
    "\n",
    "    # to make the dimension of y_train equal with x_train's\n",
    "    y_train = np.expand_dims(y_train,axis=-1)\n",
    "    \n",
    "    y_train = np.expand_dims(y_train,axis=-1)\n",
    "    \n",
    "    y_train = np.expand_dims(y_train,axis=-1)\n",
    "    \n",
    "\n",
    "    total_num = x_train.shape[0]\n",
    "    \n",
    "    train_num = int(total_num*(1-validation_split))\n",
    "    \n",
    "    x_t = x_train[:train_num]\n",
    "    \n",
    "    x_val = x_train[train_num:]\n",
    "    \n",
    "    y_t = y_train[:train_num]\n",
    "    \n",
    "    y_val = y_train[train_num:]\n",
    "    \n",
    "    print('The shape of x_train: ',x_t.shape)\n",
    "    \n",
    "    print('The shape of y_train: ',y_t.shape)\n",
    "    \n",
    "    print('The shape of x_validation: ',x_val.shape)\n",
    "    \n",
    "    print('The shape of y_validation: ',y_val.shape)\n",
    "    \n",
    "    return x_t, y_t, x_val, y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxoutConv2D(tf.keras.layers.Layer):\n",
    "    \n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        \n",
    "        super(MaxoutConv2D, self).__init__(**kwargs)\n",
    "        \n",
    "        \n",
    "    def call(self, x):\n",
    "        \n",
    "        output = K.max(x, axis=-1, keepdims=True)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        \n",
    "        input_height = input_shape[1]\n",
    "        \n",
    "        input_width = input_shape[2]\n",
    "        \n",
    "        output_height = input_height\n",
    "        \n",
    "        output_width = input_width\n",
    "        \n",
    "        return (input_shape[0], output_height, output_width, 1)\n",
    "    \n",
    "    \n",
    "# metric r2\n",
    "def r2(y_true, y_pred):\n",
    "    \n",
    "    return 1 - K.sum(K.square(y_pred - y_true))/K.sum(K.square(y_true - K.mean(y_true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dehaze_model(input_shape = INPUT_PATCH_SHAPE):\n",
    "        \n",
    "        \n",
    "    initializer = tf.initializers.RandomNormal(stddev=0.001)           # you can try different initialization strategies\n",
    "    \n",
    "    kernel_regularizer=tf.keras.regularizers.l2(WEIGHT_DECAY)\n",
    "    \n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    \n",
    "    # conv2d\n",
    "    conv_11 = tf.keras.layers.Conv2D(filters=4, kernel_size=(1, 1), strides=(1, 1), padding='same',\n",
    "                                  kernel_initializer=initializer)(inputs)\n",
    "    \n",
    "    conv_12 = tf.keras.layers.Conv2D(filters=4, kernel_size=(3, 3), strides=(1, 1), padding='same',\n",
    "                                  kernel_initializer=initializer)(inputs)\n",
    "    \n",
    "    conv_13 = tf.keras.layers.Conv2D(filters=4, kernel_size=(5, 5), strides=(1, 1), padding='same',\n",
    "                                  kernel_initializer=initializer)(inputs)\n",
    "    \n",
    "    conv_14 = tf.keras.layers.Conv2D(filters=4, kernel_size=(7, 7), strides=(1, 1), padding='same',\n",
    "                                  kernel_initializer=initializer)(inputs)\n",
    "    \n",
    "    \n",
    "    # Maxout layer output 16x16\n",
    "    max_out_1 = MaxoutConv2D()(conv_11)\n",
    "\n",
    "    max_out_2 = MaxoutConv2D()(conv_12)\n",
    "    \n",
    "    max_out_3 = MaxoutConv2D()(conv_13)\n",
    "    \n",
    "    max_out_4 = MaxoutConv2D()(conv_14)\n",
    "    \n",
    "    max_out = tf.keras.layers.Concatenate()([max_out_1, max_out_2, max_out_3, max_out_4])\n",
    "    \n",
    "    # Conv - BN - ReLU\n",
    "    # conv2 input 16x16 output 14x14\n",
    "    conv_2 = tf.keras.layers.Conv2D(filters=16, kernel_size=(3, 3), strides=(1, 1), padding='valid',\n",
    "                                  kernel_initializer=initializer)(max_out)\n",
    "    \n",
    "    conv_2 = tf.keras.layers.BatchNormalization()(conv_2)\n",
    "    \n",
    "    conv_2 = tf.keras.layers.ReLU()(conv_2)\n",
    "    \n",
    "    # Conv - BN - ReLU\n",
    "    # conv3 input 14x14 output 12x12\n",
    "    conv_3 = tf.keras.layers.Conv2D(filters=16, kernel_size=(3, 3), strides=(1, 1), padding='valid',\n",
    "                                  kernel_initializer=initializer)(conv_2)\n",
    "    \n",
    "    conv_3 = tf.keras.layers.BatchNormalization()(conv_3)\n",
    "    \n",
    "    conv_3 = tf.keras.layers.ReLU()(conv_3)\n",
    "    \n",
    "    # Conv - BN - ReLU\n",
    "    # conv4 input 12x12 output 10x10\n",
    "    conv_4 = tf.keras.layers.Conv2D(filters=16, kernel_size=(3, 3), strides=(1, 1), padding='valid',\n",
    "                                  kernel_initializer=initializer)(conv_3)\n",
    "    \n",
    "    conv_4 = tf.keras.layers.BatchNormalization()(conv_4)\n",
    "    \n",
    "    conv_4 = tf.keras.layers.ReLU()(conv_4)\n",
    "    \n",
    "    # 2x2 max pooling input 10x10 output 5x5 \n",
    "    max_pool_1 = tf.keras.layers.MaxPool2D(pool_size=(2 ,2), strides=(2, 2))(conv_4)\n",
    "    \n",
    "    # conv5 input 5x5 output 1x1\n",
    "    conv_5 = tf.keras.layers.Conv2D(filters=1, kernel_size=(5, 5), strides=(1, 1), padding='valid',\n",
    "                                  kernel_initializer=initializer)(max_pool_1)\n",
    "    \n",
    "    outputs = tf.keras.layers.ReLU()(conv_5)\n",
    " \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型训练及验证集loss和r2曲线可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ls(history):\n",
    "    plt.figure()\n",
    "    N=np.arange(EPOCHS)\n",
    "    plt.plot(N,history.history['loss'],label='train_loss')\n",
    "    plt.scatter(N,history.history['loss'])\n",
    "    plt.plot(N,history.history['val_loss'],label='val_loss')\n",
    "    plt.scatter(N,history.history['val_loss'])\n",
    "    plt.title('Training Loss on Our_dataset')\n",
    "    plt.xlabel('Epoch #')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(LOG_DIR,'training_loss.png'))\n",
    "    \n",
    "def plot_r2(history):\n",
    "    plt.figure()\n",
    "    N=np.arange(EPOCHS)\n",
    "    plt.plot(N,history.history['r2'],label='train_r2')\n",
    "    plt.scatter(N,history.history['r2'])\n",
    "    plt.plot(N,history.history['val_r2'],label='val_r2')\n",
    "    plt.scatter(N,history.history['val_r2'])\n",
    "    plt.title('Training r2 on Our_dataset')\n",
    "    plt.xlabel('Epoch #')\n",
    "    plt.ylabel('r2')\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(LOG_DIR,'training_r2.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rewrite\n",
    "def train():\n",
    "    \n",
    "    K.clear_session()\n",
    "       \n",
    "    print(\"#\" * 20)\n",
    "    \n",
    "    print(\"正在加载数据......\")\n",
    "    \n",
    "    # preparing dataset\n",
    "    x_train,y_train,x_val,y_val = create_dataset(img_dir=IMG_DIR, num_t=10)\n",
    "    \n",
    "    total_train_samples = x_train.shape[0]\n",
    "    \n",
    "    train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "    \n",
    "    val_ds = tf.data.Dataset.from_tensor_slices((x_val, y_val)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "    \n",
    "    print(\"数据加载完毕。\")\n",
    "    \n",
    "    print(\"#\" * 20)\n",
    "    \n",
    "    # optimizer\n",
    "    # use Adam with default parameters\n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "    \n",
    "    # load model\n",
    "    model = dehaze_model()\n",
    "    \n",
    "    print(\"正在保存模型结构图......\")\n",
    "    \n",
    "    tf.keras.utils.plot_model(model, to_file=os.path.join(LOG_DIR,'model.png'),\n",
    "                             show_shapes=True, show_layer_names=True)\n",
    "    \n",
    "    print(\"模型结构图保存完毕。\")\n",
    "    \n",
    "    print(\"#\" * 20)\n",
    "    \n",
    "    # model compile\n",
    "    model.compile(optimizer=optimizer,\n",
    "                 loss='mse',\n",
    "                 metrics=[r2])\n",
    "    \n",
    "    print(\"编译成功。\")\n",
    "    \n",
    "    print(\"准备训练......\")\n",
    "    \n",
    "    # Tensorboard 可视化训练过程\n",
    "    # 训练过程或完成后，可以通过在命令行输入 tensorboard --logdir <TensorBoardLog的相对地址或绝对地址>\n",
    "    # 注意查看打开的tensorboard的Data location一项地址是否指向上述地址。。。\n",
    "    # 如果你发现Data location的地址不是你输入的地址，那通过给tensorboard --logdir <TensorBoardLog的相对地址或绝对地址>这一命令\n",
    "    # 后加参数 --port:6005来解决（或其他没被占用的端口）\n",
    "    \n",
    "    tblog_dir = os.path.join(LOG_DIR, 'TensorBoardLog')\n",
    "    \n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=tblog_dir,\n",
    "                                                         histogram_freq=1)\n",
    "    \n",
    "    # early stop strategy\n",
    "    earlystop = tf.keras.callbacks.EarlyStopping(monitor='val_r2', min_delta=0.0001,\n",
    "                                                patience=2, verbose=True)\n",
    "    \n",
    "    history = model.fit(train_ds,\n",
    "                        validation_data=val_ds,\n",
    "                        epochs=EPOCHS,\n",
    "                        verbose = 1,        # only useful in interactive enviroment                        \n",
    "                        callbacks=[tensorboard_callback, earlystop])\n",
    "    \n",
    "    # metrics are stateful. they accumulate values and return a cumulative\n",
    "    # result when you call .result().\n",
    "    # reset metrics before saving so that loaded model has same state,\n",
    "    # since metric states are not preserved by Model.save_weights\n",
    "    model.reset_metrics()\n",
    "    \n",
    "    model.save(os.path.join(LOG_DIR,'model.hdf5'))\n",
    "    \n",
    "    print(\"model.hdf5 saved\")\n",
    "    \n",
    "    # save the image of loss and r2\n",
    "    # this is just to let you have an intuitive feeling. go to tensorboard for specific training information\n",
    "    plot_ls(history)\n",
    "    \n",
    "    plot_r2(history)\n",
    "    \n",
    "    print(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ ==\"__main__\":\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
